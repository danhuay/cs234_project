%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath, amssymb}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CS234: Reinforcement Learning Winter 2025 - Final Project Proposal}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%          HEADER SECTIONS HERE            %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\twocolumn[
\icmltitle{Enhancing Game Control Through \\
Hybrid Reinforcement Learning
}

\begin{icmlauthorlist}
\icmlauthor{Danhua Yan}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computer Science, Stanford University}
\icmlcorrespondingauthor{Danhua Yan}{dhyan@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{} % otherwise use the standard text.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%           MAIN SECTIONS HERE             %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
% \end{abstract}

\section{Introduction}

% What is the problem that you will be investigating ? Why is it interesting ?
% What literature have you already surveyed or will be 
% examining to provide context and background ?

Training Reinforcement Learning (RL) agents usually requires a substantial 
amount of data and exploration to find an optimal policy. In many complex 
games, the challenges of high-dimensional state spaces, sparse rewards, and 
complex dynamics make training agents using pure exploration particularly 
inefficient. Moreover, in cases where exploration opportunities are limited or 
costly, the RL agent might fail to learn any usable policies \cite{Coletti2023EffectivenessOW}. 
Such inefficiency not only slows down learning but also increases the risk of 
converging to policies that are far from optimal.

The research field of bootstrapping an RL agent's policy from demonstrations or 
imitation learning shows significant promise. Various hybrid paradigms that 
combine human guidance as offline RL and agent exploration as online RL have 
shown they can accelerate policy learning and achieve above-demonstration 
performance \cite{hester_dqfd_2017,nair_bcrl_overcoming_2018, song_hybrid_2023, 
ren_hybrid_2024, Coletti2023EffectivenessOW}.

This project investigates how hybrid RL can effectively enhance game control 
through guided explorations of the agent. It aims to evaluate the potential for 
achieving performance that surpasses the demonstration level.

\section{Approach}
\subsection{Environement}
In this project, we leverage the \texttt{stable-retro} library to create an OpenAI Gym 
environment for training an agent to play the NES game Super Mario Bros, level 1-1. The 
default integration of the environment encapsulates the game into the in-game visual 
frame as a matrix $I \in \mathbb{R}^{H \times W \times 3}$, where each element takes an 
integer value between 0 and 255, representing the RGB channels of the frame. The action 
of pressing 9 buttons on NES controllers is represented as a vector 
$\mathbf{a} = (a_1, a_2, \dots, a_9) \in \{0, 1\}^9$, where each button can be toggled 
independently, resulting in a total of 512 discrete action spaces. The default reward 
is the change in the $x$-axis position $\Delta x$ moved by Mario. In-game metadata, including 
scores, time left, and positions of Mario, can also be retrieved for each timestep $t$.

To record human demonstrations, we implemented scripts that save the gameplay 
through interactions with the environment, controlled via a game controller. 
The trajectory data of each episode $i$ is saved as $\tau_{hd}^{i} \{(s_t, a_t, 
r_t, d_t, m_t)\}_{t=0}^{T}$, where each element in the tuple represents the 
state, action, reward, termination boolean, and metadata. We recorded five 
gameplays by amateur players, each successfully finishing Level 1-1 without 
losing a life of Mario.

To frame the game into a proper RL problem that can be solved within a
reasonable time, we made below custom modifications to the default integration
of the game:

\textbf{Action Space} 
The default 512 discrete action space captures all possible joystick button 
combinations. However, most of these combinations are not meaningful for 
controlling Mario. From the human demonstration trajectories, we narrowed down 
the action space to 10 common used button combinations (see 
Appendix \ref{a1:custom_env}).

\textbf{Termination States}
The default termination of the game occurs either when Mario has exhausted all 
his lives (3 to start with) or when the time limit of 400 seconds for Level 1-1 is reached. This 
setting poses challenges for RL exploration, as it may take too long to wait 
for the game to finish if Mario gets stuck at some point. We employ a stricter 
termination state: 1) Mario has only one life, and the game terminates 
immediately if he loses it; 2) If Mario remains stuck at the same position 
without moving to the right for 10 seconds, the game is terminated.

\textbf{Reward Function}
The default reward function is simply $\mathcal{R} = \min(0,\Delta x)$, which 
awards Mario for moving right and does not penalize for moving left. To 
incorporate other elements of the game, such as collecting coins, power-ups, 
defeating enemies, and to penalize Mario for not moving or losing the game, we 
adjust the reward function as follows: 
$\mathcal{R} = \min(0,\Delta x) + \Delta k - \Delta \text{t} - \mathbf{1}[d_t = 1] p$, 
where $\Delta k$ is the score earned since the last state, $\Delta \text{t}$ is 
the time spent in seconds since the last state, and $p$ is the penalty for 
terminating the game without successfully reaching the destination.


\subsection{Baselines}
Offline-only and online-only approaches are used for 
          comparisons:
\subsubsection{Deep \textit{Q}-Learning (DQN)}
Online-only RL: Train an agent with online exploration only, 
                  leveraging Deep \textit{Q}-Learning (DQN) with $\epsilon$-greedy 
                  explorations.
\subsubsection{Behavior Cloning}
Imitation Learning Only: Train a policy via behavioral 
                  cloning (BC) using human demonstrations.

\subsection{Hybrid Reinforcement Learning (HRL)}
\subsubsection{Deep \textit{Q}-Learning from Demonstrations (DQfD)}
Following the DQfD (Deep \textit{Q}-Learning from 
                  Demonstrations) framework by \cite{hester_dqfd_2017}, which 
                  incorporates expert demonstrations into the replay buffer of 
                  DQN to control explorations.

\section{Experiements}

\section{Next Steps}
Leveraging behavioral cloning (BC) as a warm-start, then 
                  further leveraging PPO (Proximal Policy Optimization) for 
                  policy fine-tuning. This approach is inspired by 
                  \cite{Coletti2023EffectivenessOW}.




\bibliography{ref}
\bibliographystyle{icml2018}

\clearpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Custom Environment}
\label{a1:custom_env}

The default 512 discrete action space captures all possible joystick button 
combinations. However, most of these combinations are not meaningful for 
controlling Mario. From the human demonstration trajectories, we narrowed down 
the action space to 10 common used button combinations.
Then the action vector is labeled as integers (0-9, following below orders)
as discrete action space for the environment.

\begin{verbatim}
# List of meaningful button combinations used in gameplay
meaningful_actions = [
      [0, 0, 0, 0, 0, 0, 0, 0, 0],  # No action
      [0, 0, 0, 0, 0, 0, 0, 0, 1],  # A (Jump)
      [0, 0, 0, 0, 0, 0, 0, 1, 0],  # Right
      [0, 0, 0, 0, 0, 0, 0, 1, 1],  # Right + A (Jump)
      [0, 0, 0, 0, 0, 0, 1, 0, 0],  # Left
      [0, 0, 0, 0, 0, 0, 1, 0, 1],  # Left + A (Jump)
      [1, 0, 0, 0, 0, 0, 0, 0, 0],  # B (Run)
      [1, 0, 0, 0, 0, 0, 0, 0, 1],  # A + B (Jump + Run)
      [1, 0, 0, 0, 0, 0, 0, 1, 0],  # Right + B (Run)
      [1, 0, 0, 0, 0, 0, 0, 1, 1]   # Right + A + B (Jump + Run)
]
\end{verbatim}

\end{document}


["NULL"],
["A"],
["RIGHT"],
["RIGHT", "A"],
["LEFT"],
["LEFT", "A"],
["B"],
["A", "B"],
["RIGHT", "B"],
["RIGHT", "A", "B"],