%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2018}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{CS234: Reinforcement Learning Winter 2025 - Final Project Proposal}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%          HEADER SECTIONS HERE            %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\twocolumn[
\icmltitle{Enhancing Game Control Through \\
Hybrid Reinforcement Learning
}

\begin{icmlauthorlist}
\icmlauthor{Danhua Yan}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computer Science, Stanford University}
\icmlcorrespondingauthor{Danhua Yan}{dhyan@stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{} % otherwise use the standard text.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%           MAIN SECTIONS HERE             %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
% \end{abstract}

\section{Background and Related Work}

% What is the problem that you will be investigating ? Why is it interesting ?
% What literature have you already surveyed or will be 
% examining to provide context and background ?

Training Reinforcement Learning (RL) agents solely from exploration is usually 
data inefficient and likely converges to sub-optimal policies. 
The research field of bootstraping an RL agent's policy from demonstrations or 
imitation learning shows significant promise. 
Various hybrid paradigms that combine human guidance as offline RL and agent 
exploration as online RL have shown they can accelerate policy learning and 
achieve above-demonstration performance.
\cite{hester_dqfd_2017,nair_bcrl_overcoming_2018, song_hybrid_2023, ren_hybrid_2024, 
Coletti2023EffectivenessOW}.

This project investigates how hybrid RL can effectively enhance game control 
through guided explorations of the agent. It aims to evaluate the potential 
for achieving performance that surpasses the demonstration level.


\section{Data}
% If relevant, what data, simulator or real world RL domain will you be looking at ? 
% If you are collecting new datasets, how do you plan to collect them ?
This project will leverage the \texttt{gym-super-mario-bros} library to create 
an OpenAI Gym environment for training an agent to play the NES game Super Mario Bros. The states are represented 
by in-game visual frames, the actions are discrete game controls, and the 
rewards are the game's scoring systems. Human demonstrations will be recorded 
into offline trajectories, and the agent will perform online exploration in 
the environment.


\section{Methods}
% What method, algorithm or theoretical analysis are you proposing ? 
% If there are existing implementations, will you use them and how ? 
% How do you plan to improve or modify such implementations ? 
% If you are addressing a theoretical question, how do you plan to make progress ?
The methodology involves assessing RL agent performance on baseline and hybrid 
approaches on the trained game level and an unseen similar level to compare 
generalization capabilities.

\begin{itemize}
    \item Baselines: Offline-only and online-only approaches are used for 
          comparisons:
        \begin{itemize}
            \item Imitation Learning Only: Train a policy via behavioral 
                  cloning (BC) using human demonstrations.
            \item Online-only RL: Train an agent with online exploration only, 
                  leveraging Deep \textit{Q}-Learning (DQN) with $\epsilon$-greedy 
                  explorations.
        \end{itemize}
    \item Hybrid RL: We propose two paradigms of hybrid approaches:
          \begin{itemize}
            \item Following the DQfD (Deep \textit{Q}-Learning from 
                  Demonstrations) framework by \cite{hester_dqfd_2017}, which 
                  incorporates expert demonstrations into the replay buffer of 
                  DQN to control explorations.
            \item Leveraging behavioral cloning (BC) as a warm-start, then 
                  further leveraging PPO (Proximal Policy Optimization) for 
                  policy fine-tuning. This approach is inspired by 
                  \cite{Coletti2023EffectivenessOW}.
          \end{itemize}
\end{itemize}

\section{Evaluations}
% How will you evaluate your results ? 
% Qualitatively, what kind of results do you expect (e.g. plots or figures) ? 
% Quantitatively, what kind of analysis will you use to evaluate and/or 
% compare your results (e.g. what performance metrics or statistical tests) ?
We will evaluate the approaches using both quantitative and qualitative metrics. 
Quantitatively, performance will be measured via cumulative reward, level 
completion rate, and distance traversed per episode, plotted as learning curves 
against training episodes or timesteps. Multiple independent runs will ensure 
statistical significance. Sample efficiency will be analyzed by measuring 
interactions required to reach performance thresholds and wall-clock training 
time. Qualitatively, gameplay visualizations and trajectory overlays will 
provide insights into behavioral strategies.


% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


\bibliography{ref}
\bibliographystyle{icml2018}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
