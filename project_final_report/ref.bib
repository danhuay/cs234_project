@article{Coletti2023EffectivenessOW,
  title={Effectiveness of Warm-Start PPO for Guidance with Highly Constrained Nonlinear Fixed-Wing Dynamics},
  author={Christian T. Coletti and Kyle A. Williams and Hannah C. Lehman and Zahi M. Kakish and Daniel Whitten and Julie Parish},
  journal={2023 American Control Conference (ACC)},
  year={2023},
  pages={3288-3295},
  url={https://api.semanticscholar.org/CorpusID:259338376}
}


@misc{hester_dqfd_2017,
	title = {Deep {Q}-learning from {Demonstrations}},
	url = {http://arxiv.org/abs/1704.03732},
	doi = {10.48550/arXiv.1704.03732},
	abstract = {Deep reinforcement learning (RL) has achieved several high proﬁle successes in difﬁcult decision-making problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages small sets of demonstration data to massively accelerate the learning process even from relatively small amounts of demonstration data and is able to automatically assess the necessary ratio of demonstration data while learning thanks to a prioritized replay mechanism. DQfD works by combining temporal difference updates with supervised classiﬁcation of the demonstrator’s actions. We show that DQfD has better initial performance than Prioritized Dueling Double Deep Q-Networks (PDD DQN) as it starts with better scores on the ﬁrst million steps on 41 of 42 games and on average it takes PDD DQN 83 million steps to catch up to DQfD’s performance. DQfD learns to out-perform the best demonstration given in 14 of 42 games. In addition, DQfD leverages human demonstrations to achieve state-of-the-art results for 11 games. Finally, we show that DQfD performs better than three related algorithms for incorporating demonstration data into DQN.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and Leibo, Joel Z. and Gruslys, Audrunas},
	month = nov,
	year = {2017},
	note = {arXiv:1704.03732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\danhua\\Zotero\\storage\\PFSX5YW9\\Hester et al. - 2017 - Deep Q-learning from Demonstrations.pdf:application/pdf},
}

@misc{nair_bcrl_overcoming_2018,
	title = {Overcoming {Exploration} in {Reinforcement} {Learning} with {Demonstrations}},
	url = {http://arxiv.org/abs/1709.10089},
	doi = {10.48550/arXiv.1709.10089},
	abstract = {Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, ﬁnding a non-zero reward is exponentially more difﬁcult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
	month = feb,
	year = {2018},
	note = {arXiv:1709.10089 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {PDF:C\:\\Users\\danhua\\Zotero\\storage\\IVBD9K89\\Nair et al. - 2018 - Overcoming Exploration in Reinforcement Learning with Demonstrations.pdf:application/pdf},
}

@misc{song_hybrid_2023,
	title = {Hybrid {RL}: {Using} {Both} {Offline} and {Online} {Data} {Can} {Make} {RL} {Efficient}},
	shorttitle = {Hybrid {RL}},
	url = {http://arxiv.org/abs/2210.06718},
	doi = {10.48550/arXiv.2210.06718},
	abstract = {We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an ofﬂine dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure ofﬂine and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efﬁcient whenever the ofﬂine dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outperforms state-of-the-art online, ofﬂine, and hybrid RL baselines on challenging benchmarks, including Montezuma’s Revenge.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Song, Yuda and Zhou, Yifei and Sekhari, Ayush and Bagnell, J. Andrew and Krishnamurthy, Akshay and Sun, Wen},
	month = mar,
	year = {2023},
	note = {arXiv:2210.06718 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\danhua\\Zotero\\storage\\YP7NLWMA\\Song et al. - 2023 - Hybrid RL Using Both Offline and Online Data Can Make RL Efficient.pdf:application/pdf},
}

@misc{ren_hybrid_2024,
	title = {Hybrid {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2402.08848},
	doi = {10.48550/arXiv.2402.08848},
	abstract = {The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert’s. In this work, we propose using hybrid RL – training on a mixture of online and expert data – to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn’t need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formally, we derive a reduction from inverse RL to expert-competitive RL (rather than globally optimal RL) that allows us to dramatically reduce interaction during the inner policy search loop while maintaining the benefits of the IRL approach. This allows us to derive both model-free and model-based hybrid inverse RL algorithms with strong policy performance guarantees. Empirically, we find that our approaches are significantly more sample efficient than standard inverse RL and several other baselines on a suite of continuous control tasks.},
	language = {en},
	urldate = {2025-02-10},
	publisher = {arXiv},
	author = {Ren, Juntao and Swamy, Gokul and Wu, Zhiwei Steven and Bagnell, J. Andrew and Choudhury, Sanjiban},
	month = jun,
	year = {2024},
	note = {arXiv:2402.08848 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\danhua\\Zotero\\storage\\RZBPH3F7\\Ren et al. - 2024 - Hybrid Inverse Reinforcement Learning.pdf:application/pdf},
}

@misc{pytorch_mario_rl_tutorial,
  author       = {PyTorch},
  title        = {Reinforcement Learning (PPO) with Super Mario Bros},
  year         = {2024},
  url          = {https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html},
  note         = {Accessed: 2025-03-15}
}
